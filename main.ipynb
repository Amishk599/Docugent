{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9107d07b-14de-41f6-bb64-adf7225f1371",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import time\n",
    "from watchdog.observers import Observer\n",
    "from watchdog.events import FileSystemEventHandler\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "import requests\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional\n",
    "import json\n",
    "from uuid import uuid4\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models.base import BaseChatModel\n",
    "from langchain.schema import (\n",
    "    Document,\n",
    "    ChatMessage,\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ChatResult\n",
    ")\n",
    "\n",
    "from langchain_core.messages import (\n",
    "    BaseMessage,\n",
    ")\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_core.output_parsers import StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b262574e-8bd2-4e33-82b9-0d0ffadbc56e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amish/Workspace/personal/ai/docugent/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "stop_watcher = False\n",
    "directory_to_watch = \"/Users/amish/Workspace/personal/ai/docugent/documents\"\n",
    "# Load a local embedding model\n",
    "# embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "# embedding_dimension is 768\n",
    "embedding_dimension = len(embedding_model.embed_query(\"hello world\"))\n",
    "# Create a FAISS index\n",
    "index = faiss.IndexFlatL2(embedding_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d26446d-eefa-484f-ac4d-17b728e7b017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-memory dictionary to store content and metadata of all pdfs. Contains complete contents of all PDFs\n",
    "repository = {}\n",
    "# initialise vector store for storing embeddings\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embedding_model,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3a1744f-1dfb-453d-a15a-4adab9c3a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Processing PDF: {file_path}\")\n",
    "        reader = PdfReader(file_path)\n",
    "        content = \"\\n\".join([page.extract_text() for page in reader.pages])\n",
    "        # update the in-memory store\n",
    "        repository[os.path.basename(file_path)] = {\n",
    "            \"path\": file_path,\n",
    "            \"last_modified\": os.path.getmtime(file_path),\n",
    "            \"content\": content,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1eea7617-a1c6-4cb2-b00c-73aa09010900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any files already present in directory is processed\n",
    "def process_existing_files_in_directory(directory_to_watch):\n",
    "    for file_name in os.listdir(directory_to_watch):\n",
    "        file_path = os.path.join(directory_to_watch, file_name)\n",
    "        if os.path.isfile(file_path) and file_name.endswith(\".pdf\"):\n",
    "            print(f\"Processing existing PDF: {file_path}\")\n",
    "            process_pdf(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "267a7e58-f74e-48ea-a88d-61dc94eb8c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom handler\n",
    "class PDFHandler(FileSystemEventHandler):\n",
    "    def __init__(self, process_pdf_callback):\n",
    "        self.process_pdf_callback = process_pdf_callback\n",
    "\n",
    "    def on_create(self, event):\n",
    "        if event.is_directory:\n",
    "            return\n",
    "        if event.src_path.endswith(\".pdf\"):\n",
    "            print(f\"New PDF Added: {event.src_path}\")\n",
    "            self.process_pdf_callback(event.src_path)\n",
    "\n",
    "    def on_modified(self, event):\n",
    "        if event.is_directory:\n",
    "            return\n",
    "        if event.src_path.endswith(\".pdf\"):\n",
    "            print(f\"PDF Modified: {event.src_path}\")\n",
    "            self.process_pdf_callback(event.src_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "064408df-a5c1-43c6-b6c7-00b50d0c7ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory watcher function\n",
    "def start_directory_watcher_with_initial_scan(directory_to_watch):\n",
    "    global stop_watcher\n",
    "    \n",
    "    # first, process existing files\n",
    "    process_existing_files_in_directory(directory_to_watch)\n",
    "    \n",
    "    # second, start directory watcher\n",
    "    event_handler = PDFHandler(process_pdf)\n",
    "    observer = Observer()\n",
    "    observer.schedule(event_handler, path=directory_to_watch, recursive=False)\n",
    "    observer.start()\n",
    "    try:\n",
    "        print(f\"Watching directory: {directory_to_watch}\")\n",
    "        while not stop_watcher:\n",
    "            time.sleep(2) # keep the observer running\n",
    "    except KeyboardInterrupt:\n",
    "         print(\"KeyboardInterrupt detected. Stopping observer...\")\n",
    "    finally:\n",
    "        # Ensure the observer stops when the loop exits for any reason\n",
    "        observer.stop()\n",
    "        observer.join()\n",
    "        print(f\"Stopped Watching directory: {directory_to_watch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "794f338a-b248-420f-a777-3170611d9512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run observer in a seperate thread\n",
    "def start_watching_in_thread(directory_to_watch):\n",
    "    watcher_thread = threading.Thread(target=start_directory_watcher_with_initial_scan, args=(directory_to_watch,), daemon=True)\n",
    "    watcher_thread.start()\n",
    "    return watcher_thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0117ff00-35d8-44bc-b2ac-0742e4b78a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing existing PDF: /Users/amish/Workspace/personal/ai/docugent/documents/LRA.pdf\n",
      "Processing PDF: /Users/amish/Workspace/personal/ai/docugent/documents/LRA.pdf\n",
      "Processing existing PDF: /Users/amish/Workspace/personal/ai/docugent/documents/Receipts_Delta_AirLines.pdf\n",
      "Processing PDF: /Users/amish/Workspace/personal/ai/docugent/documents/Receipts_Delta_AirLines.pdf\n",
      "Watching directory: /Users/amish/Workspace/personal/ai/docugent/documents\n"
     ]
    }
   ],
   "source": [
    "# start the watcher in a thread\n",
    "watcher_thread = start_watching_in_thread(directory_to_watch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9eb9bd15-46f5-46f7-9874-14f156136277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start/stop watcher knob\n",
    "stop_watcher = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f0891ab-3222-4e0a-919f-ade6fd8c222c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# current value of knob\n",
    "stop_watcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ea61040-1dc4-49b0-b227-84e0849ee2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(content, chunk_size=1000, chunk_overlap=200):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \"],\n",
    "    )\n",
    "    return text_splitter.create_documents([content])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "07aba6d8-b3b5-43f4-b4ce-a2f96568017f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = chunk_text(repository['LRA.pdf']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c561ef68-fda5-4295-a35a-ead2985b2366",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into chunks using LangChain's RecursiveCharacterTextSplitter\n",
    "def chunk_and_add_metadata(file_name, content, chunk_size=1000, chunk_overlap=200):\n",
    "    chunks = chunk_text(content, chunk_size, chunk_overlap)\n",
    "    return [\n",
    "        Document(page_content=chunk.page_content, metadata={\"file_name\": file_name, **chunk.metadata})\n",
    "        for chunk in chunks\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4d9c272-15c3-4dd7-a5e8-26f8745be505",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_documents = chunk_and_add_metadata('LRA.pdf', repository['LRA.pdf']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acb32b5f-2120-471f-9980-7cc5a83c0489",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'file_name': 'LRA.pdf'}, page_content='I485/ GC LRAs/ 11.20.24 /HAF \\n \\n \\n10451 Mill Run Circle, Suite 100, Owings Mills, Maryland 21117  \\n \\nTel: 410.356.5440 | Fax: 410.356.5669 | Web: www.murthy.com  | eM ail: law@murthy.com  \\n \\n \\n \\nPRIVILEGED AND CONFIDENTIAL  \\nCLIENT COMMUNICATION \\nATTORNEY WORK P RODUCT  \\n  Hello :  \\n \\nThank you for contacting the Murthy Law Firm in connection with processing your immigration case/s. For \\ny\\nour convenience, we have attached the standard Legal Representation Agreement  for your review and \\nsignature, so that we may officially represent you. We are honored and delighted to help you.  \\n \\nOur experienced and knowledgeable team will work to assist you. Our firm is able to provide you with many \\nv\\naluable online resources. Among these are: our award- winning website, MurthyDotCom; the M urthyChat , \\nthrough which we provide answers to your questions in real time; the MurthyForum, where our attorneys')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3abaef2-03c8-4c56-babf-38f5365fc0e1",
   "metadata": {},
   "source": [
    "#### NOTE : this function was made to work with sentence transformer for generting embeddings of the Documents\n",
    "```\n",
    "def generate_embeddings_for_all_documents(repository):\n",
    "    all_embeddings_with_metadata = []\n",
    "    for file_name, doc in repository.items():\n",
    "        chunked_documents = chunk_and_add_metadata(file_name, doc['content'])\n",
    "        # Generate embeddings for the chunks\n",
    "        embeddings = embedding_model.encode([doc.page_content for doc in chunked_documents])\n",
    "        embeddings_with_metadata = [\n",
    "            {\"embedding\": embedding, \"metadata\": doc.metadata}\n",
    "            for embedding, doc in zip(embeddings, chunked_documents)\n",
    "        ]\n",
    "        # Append to the final list\n",
    "        all_embeddings_with_metadata.extend(embeddings_with_metadata)\n",
    "        print(f\"File: {file_name}, chunks:[{len(chunked_documents)}], embeddings:[{len(embeddings_with_metadata)}]\")\n",
    "    return all_embeddings_with_metadata\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027bfe09-aa79-4a77-ae89-5e542f371c1c",
   "metadata": {},
   "source": [
    "#### NOTE : this function was made to work with sentence transformer for generting embeddings of the Documents\n",
    "```\n",
    "all_embeddings = generate_embeddings_for_all_documents(repository)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7071df3-ea1a-4316-953b-018e98aed357",
   "metadata": {},
   "source": [
    "#### NOTE : this function was made to work with sentence transformer for generting embeddings of the Documents\n",
    "#### add all_emb|eddings to FAISS index\n",
    "```\n",
    "embedding_vectors = np.array([item['embedding'] for item in all_embeddings]).astype('float32')\n",
    "index.add(embedding_vectors)\n",
    "print(f\"Added {index.ntotal} embeddings to the FAISS index.\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f5d63-5085-45fd-99b1-343e6ecd320f",
   "metadata": {},
   "source": [
    "#### NOTE : this function was made to work with sentence transformer for generting embeddings of the Documents\n",
    "#### Encode the query\n",
    "```\n",
    "query = \"What resources does Murthy Law Firm provides\"\n",
    "query_embedding = embedding_model.encode([query]).astype('float32')\n",
    "```\n",
    "\n",
    "#### Search the index\n",
    "```\n",
    "k = 5  # Number of nearest neighbors\n",
    "distances, indices = index.search(query_embedding, k)\n",
    "```\n",
    "\n",
    "#### Retrieve results with metadata\n",
    "```\n",
    "results = [all_embeddings[i] for i in indices[0]]\n",
    "for result in results:\n",
    "    print(f\"File: {result['metadata']['file_name']}, Distance: {distances[0][0]}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "16a1d783-f470-4245-922d-86fa9ca7ca41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the documents/embedding to the vector store\n",
    "def embed_all_PDFs_and_add_to_vector_store(repository):\n",
    "    for file_name, doc in repository.items():\n",
    "        chunked_documents = chunk_and_add_metadata(file_name, doc['content'])\n",
    "        uuids = [str(uuid4()) for _ in range(len(chunked_documents))]\n",
    "        vector_store.add_documents(documents=chunked_documents, ids=uuids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bf1801-7e4d-4d7d-9c62-15fd3d4bb999",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_all_PDFs_and_add_to_vector_store(repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0616da-1046-42e4-be9e-d0ba6ef20cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try retrieving records\n",
    "results = vector_store.similarity_search(query=\"Passangers travelling from Los Angeles to Bangalore\",k=1)\n",
    "for doc in results:\n",
    "    print(f\"* {doc.page_content} [{doc.metadata}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754c712b-b9d1-42e6-ba34-1c2ef0e6865d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement a custom chat model\n",
    "class ChatLocalOllamaMistral(BaseChatModel):\n",
    "    \"\"\"\n",
    "    A custom chat model that interfaces with a locally installed Ollama Mistral model\n",
    "    \"\"\"\n",
    "    model_name: str = Field(default=\"mistral\", alias=\"model\")\n",
    "    host: str = \"http://localhost:11434\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"local-ollama-mistral\"\n",
    "\n",
    "    def _generate(self, message: List[BaseMessage], stop: Optional[List[str]] = None,) -> ChatResult:\n",
    "        \"\"\"\n",
    "        Process a list of messages and generate a response\n",
    "        \"\"\"\n",
    "        formatted_message = self._format_message(message)\n",
    "        rsp = self._send_request(formatted_message)\n",
    "        # print(f\"Raw response from /api/generate : \\n{rsp}\\n\")\n",
    "        # form ai message for the received response\n",
    "        ai_message = AIMessage(\n",
    "            content=rsp.get(\"response\", \"\"),\n",
    "            additional_kwargs={\n",
    "                \"model_name\": rsp.get(\"model\", \"\"),\n",
    "            }\n",
    "        )\n",
    "        # return finally as a ChatResult\n",
    "        return ChatResult(\n",
    "            generations=[ChatGeneration(message=ai_message)]\n",
    "        )\n",
    "            \n",
    "    def _send_request(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Sends a request to the local Ollama Mistral API\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "        }\n",
    "        # TODO : Add retries\n",
    "        try:\n",
    "            response = requests.post(\n",
    "                f\"{self.host}/api/generate\",\n",
    "                json=payload,\n",
    "            )\n",
    "            # print(f\"response from API: {response.text}\")\n",
    "            response.raise_for_status()\n",
    "            return response.json()\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Request failed with error: {e}\")\n",
    "            raise RuntimeError(\"Failed to generate response\")\n",
    "\n",
    "    def _format_message(self, messages: List[ChatMessage]) -> str:\n",
    "        \"\"\"\n",
    "        Convert LangChain messages into a single string format\n",
    "        \"\"\"\n",
    "        formatted_messages = \"\"\n",
    "        for message in messages:\n",
    "            if isinstance(message, AIMessage):\n",
    "                formatted_messages += f\"AI: {message.content}\\n\"\n",
    "            else:\n",
    "                formatted_messages += f\"User: {message.content}\\n\"\n",
    "        return formatted_messages.strip()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c6c142-b502-49e3-a373-11a722f80b86",
   "metadata": {},
   "source": [
    "llm = ChatLocalOllamaMistral(model_name=\"mistral\")\n",
    "message = [[HumanMessage(content=\"Hello\")]]\n",
    "response = llm.generate(message)\n",
    "print(response.generations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc66698-6870-4ec5-b9a2-d35d6b38e7c4",
   "metadata": {},
   "source": [
    "# Load FAISS vectore store retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
    "relevant_docs = retriever.invoke(\"dispute resolution\")\n",
    "# relevant_docs\n",
    "context = \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d2c97-a388-4085-9cb2-38a744a41d0d",
   "metadata": {},
   "source": [
    "# Define the prompt template for RAG\n",
    "template = \"\"\"Use the following pieces of context to answer the question. If you don't know the answer, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "question1 = \"What all resources does murthy law firm offers?\"\n",
    "question2 = \"Where is murthy law firm located?\"\n",
    "question3 = \"can you give me a summary of terms and conditions of the Legal Representation Agreement?\"\n",
    "question4 = \"what all is present in the addendum?\"\n",
    "question5 = \"tell me about dispute resolution\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "chain = LLMChain( llm=llm, prompt=prompt)\n",
    "# answer = chain.run({\"context\": context, \"question\": \"What resources does Murthy Law Firm offer?\"})\n",
    "\n",
    "response = chain.run(\n",
    "    context=context, \n",
    "    question=question5\n",
    ")\n",
    "\n",
    "print(\"Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef92fb0-93ef-4c35-a7c9-0de43c020a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatLocalOllamaMistral(model_name=\"mistral\")\n",
    "# Load FAISS vectore store retriever\n",
    "retriever = vector_store.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 1})\n",
    "# Define the prompt template for RAG\n",
    "template = \"\"\"Use the following pieces of context to answer the question. If you don't know the answer, say so.\n",
    "Context:\n",
    "{context}\n",
    "Question:\n",
    "{question}\n",
    "Answer:\"\"\"\n",
    "prompt = PromptTemplate(template=template, input_variables=[\"context\", \"question\"])\n",
    "\n",
    "def rag_pipeline(question):\n",
    "    relevant_docs = retriever.invoke(question)\n",
    "    print(relevant_docs)\n",
    "    chain = LLMChain( llm=llm, prompt=prompt)\n",
    "    response = chain.run(context=context, question=question5)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69bbad7-783d-455b-a1a3-fe5b1c989713",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Where is murthy law firm located?\"\n",
    "response = rag_pipeline(question)\n",
    "print(\"Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee83c2a-bc46-4037-b749-d33a1de6dcf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python-3.10.12 (venv-1)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
